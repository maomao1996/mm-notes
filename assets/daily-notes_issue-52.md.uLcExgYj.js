import{_ as s,h as e,a9 as i,o as l}from"./chunks/framework.iNNLCK4S.js";const b=JSON.parse(`{"title":"本地部署 DeepSeek 大模型","description":"","frontmatter":{"head":[["script",{"charset":"UTF-8","id":"LA_COLLECT","src":"//sdk.51.la/js-sdk-pro.min.js"}],["script",{},"typeof LA !== 'undefined' && LA.init({\\"id\\":\\"3LNfUkScYzEz6k4D\\",\\"ck\\":\\"3LNfUkScYzEz6k4D\\",\\"hashMode\\":true})"]]},"headers":[],"relativePath":"daily-notes/issue-52.md","filePath":"daily-notes/issue-52.md","lastUpdated":null}`),n={name:"daily-notes/issue-52.md"};function t(r,a,h,p,o,k){return l(),e("div",null,a[0]||(a[0]=[i(`<h1 id="本地部署-deepseek-大模型" tabindex="-1">本地部署 DeepSeek 大模型 <a class="header-anchor" href="#本地部署-deepseek-大模型" aria-label="Permalink to &quot;本地部署 DeepSeek 大模型&quot;">​</a></h1><div class="tip custom-block"><p class="custom-block-title">原文地址</p><p><a href="https://github.com/maomao1996/daily-notes/issues/52" target="_blank" rel="noreferrer">本地部署 DeepSeek 大模型 | GitHub</a></p></div><p>最近 DeepSeek 老转圈圈和系统繁忙，所以决定自己部署一个</p><h2 id="下载和安装-ollama" tabindex="-1">下载和安装 Ollama <a class="header-anchor" href="#下载和安装-ollama" aria-label="Permalink to &quot;下载和安装 Ollama&quot;">​</a></h2><p>Ollama 是一个强大的开源框架，旨在为本地运行大型语言模型提供便利。通过简单的安装指令，用户可以快速在本地运行开源大型语言模型</p><h3 id="安装-ollama" tabindex="-1">安装 Ollama <a class="header-anchor" href="#安装-ollama" aria-label="Permalink to &quot;安装 Ollama&quot;">​</a></h3><ol><li>打开 <a href="https://ollama.com/download" target="_blank" rel="noreferrer">Ollama 官网</a></li><li>下载对应系统的安装包</li><li>安装并启动 Ollama</li></ol><h3 id="ollama-常用命令" tabindex="-1">Ollama 常用命令 <a class="header-anchor" href="#ollama-常用命令" aria-label="Permalink to &quot;Ollama 常用命令&quot;">​</a></h3><div class="language-sh vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">sh</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 在命令行中运行模型</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> &lt;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">模型名</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">称</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 列出可用模型</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> list</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 查看模型状态</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ps</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 删除模型</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> rm</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> &lt;</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">模型名</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">称</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 启动 API 服务</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> serve</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><h2 id="部署-deepseek-r1-大模型" tabindex="-1">部署 DeepSeek R1 大模型 <a class="header-anchor" href="#部署-deepseek-r1-大模型" aria-label="Permalink to &quot;部署 DeepSeek R1 大模型&quot;">​</a></h2><blockquote><p>部署 DeepSeek R1 大模型前，需要确保系统具备足够的内存和存储空间（本人电脑为 M3 Pro 36GB 内存，运行 14B 模型没啥压力，运行 32B 模型会卡顿）</p></blockquote><ol><li>打开 <a href="https://ollama.com/library/deepseek-r1" target="_blank" rel="noreferrer">DeepSeek R1 | Ollama</a></li><li>复制相应的模型安装指令</li><li>在终端中运行模型安装指令</li></ol><div class="language-sh vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">sh</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 下载 deepseek-r1 8b 模型</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">ollama</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> deepseek-r1:8b</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h3 id="解决-ollama-下载模型速度变慢" tabindex="-1">解决 Ollama 下载模型速度变慢 <a class="header-anchor" href="#解决-ollama-下载模型速度变慢" aria-label="Permalink to &quot;解决 Ollama 下载模型速度变慢&quot;">​</a></h3><p>如果在下载模型时速度变慢，可以使用 <code>Ctrl + C</code> 取消下载，然后重新运行下载指令。</p><p><img src="https://cdn.jsdelivr.net/gh/maomao1996/daily-notes/static/52/52-1.png" alt="解决 Ollama 下载模型速度变慢" loading="lazy"></p><ul><li><a href="https://github.com/ollama/ollama/issues/3794" target="_blank" rel="noreferrer">模型下载最后 1%速度骤降，导致下载时间超长</a></li></ul><h2 id="接入到-webui-中" tabindex="-1">接入到 WebUI 中 <a class="header-anchor" href="#接入到-webui-中" aria-label="Permalink to &quot;接入到 WebUI 中&quot;">​</a></h2><p>接入 WebUI 有多种方式：</p><ol><li>自行搭建 WebUI</li><li>使用浏览器插件接入</li><li>使用第三方提供的 WebUI</li></ol><h3 id="自行搭建-webui" tabindex="-1">自行搭建 WebUI <a class="header-anchor" href="#自行搭建-webui" aria-label="Permalink to &quot;自行搭建 WebUI&quot;">​</a></h3><ol><li>下载并安装 <a href="https://www.docker.com/" target="_blank" rel="noreferrer">Docker</a></li><li>使用 Docker 运行 <a href="https://github.com/open-webui/open-webui" target="_blank" rel="noreferrer">Open WebUI</a></li></ol><div class="language-sh vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">sh</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">docker</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> run</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -d</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -p</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> 3000:8080</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --add-host=host.docker.internal:host-gateway</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -v</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> open-webui:/app/backend/data</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --name</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> open-webui</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --restart</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> always</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ghcr.io/open-webui/open-webui:main</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><ol start="3"><li>在浏览器中访问 <a href="http://localhost:3000" target="_blank" rel="noreferrer">http://localhost:3000</a></li></ol><hr><ul><li><a href="https://github.com/open-webui/open-webui" target="_blank" rel="noreferrer">Open WebUI | GitHub</a></li></ul><h3 id="使用浏览器插件" tabindex="-1">使用浏览器插件 <a class="header-anchor" href="#使用浏览器插件" aria-label="Permalink to &quot;使用浏览器插件&quot;">​</a></h3><ol><li>安装浏览器扩展 —— <a href="https://chromewebstore.google.com/detail/page-assist-%E6%9C%AC%E5%9C%B0-ai-%E6%A8%A1%E5%9E%8B%E7%9A%84-web/jfgfiigpkhlkbnfnbobbkinehhfdhndo" target="_blank" rel="noreferrer">Page Assist - 本地 AI 模型的 Web UI</a></li><li>在浏览器中访问 <code>chrome-extension://jfgfiigpkhlkbnfnbobbkinehhfdhndo/options.html</code></li></ol><hr><ul><li><a href="https://github.com/n4ze3m/page-assist" target="_blank" rel="noreferrer">page-assist | GitHub</a></li><li><a href="https://chromewebstore.google.com/detail/page-assist-%E6%9C%AC%E5%9C%B0-ai-%E6%A8%A1%E5%9E%8B%E7%9A%84-web/jfgfiigpkhlkbnfnbobbkinehhfdhndo" target="_blank" rel="noreferrer">Page Assist - 本地 AI 模型的 Web UI | Chrome Web Store</a></li></ul><h3 id="使用第三方提供的-webui" tabindex="-1">使用第三方提供的 WebUI <a class="header-anchor" href="#使用第三方提供的-webui" aria-label="Permalink to &quot;使用第三方提供的 WebUI&quot;">​</a></h3><blockquote><p>以 <a href="https://web.chatboxai.app" target="_blank" rel="noreferrer">Chatbox</a> 为例</p></blockquote><h4 id="设置环境变量" tabindex="-1">设置环境变量 <a class="header-anchor" href="#设置环境变量" aria-label="Permalink to &quot;设置环境变量&quot;">​</a></h4><p>默认情况下，Ollama 只允许本地访问。如果希望其他网站也能访问，需要设置环境变量 <code>OLLAMA_ORIGINS</code>。</p><ol><li>打开终端</li><li>运行以下命令（以 MacOS 和 zsh 为例）</li></ol><div class="language-sh vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">sh</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 允许所有来源访问</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">echo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;export OLLAMA_ORIGINS=&quot;*&quot;&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> &gt;&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> $HOME</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">/.zshrc</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><ol start="3"><li>关闭并重新启动 Ollama 软件</li></ol><h4 id="使用-chatbox" tabindex="-1">使用 Chatbox <a class="header-anchor" href="#使用-chatbox" aria-label="Permalink to &quot;使用 Chatbox&quot;">​</a></h4><blockquote><p>以 <a href="https://web.chatboxai.app" target="_blank" rel="noreferrer">Chatbox</a> 为例</p></blockquote><ol><li>在浏览器中访问 <a href="https://web.chatboxai.app" target="_blank" rel="noreferrer">Chatbox</a></li><li>点击 <code>设置</code></li><li>将模型提供方设置为 <code>OLLAMA API</code>（选择后会自动获取到本地的 Ollama 模型）</li><li>点击 <code>保存</code></li></ol>`,40)]))}const c=s(n,[["render",t]]);export{b as __pageData,c as default};
